{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tim/Desktop/Speech/lab1')\n",
    "sys.path.append('/Users/tim/Desktop/Speech/lab2')\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "from lab3_tools import *\n",
    "from lab3_proto import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sil_0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phoneHMMs is a dictionary with 21 keys, each corresponding to a phonetic model\n",
    "phoneHMMs = np.load('../lab2/lab2_models_all.npz', allow_pickle=True)['phoneHMMs'].item()\n",
    "phones = sorted(phoneHMMs.keys())\n",
    "nstates = {phone: phoneHMMs[phone]['means'].shape[0] for phone in phones}\n",
    "# A list of unique states for reference\n",
    "# Note that we model three segments for each phoneme\n",
    "stateList = [ph + '_' + str(id) for ph in phones for id in range(nstates[ph])]\n",
    "stateList[39]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forced Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the audio and compute liftered MFCC features\n",
    "from lab1_proto import mfcc\n",
    "\n",
    "filename = 'tidigits/disc_4.1.1/tidigits/train/man/nw/z43a.wav'\n",
    "samples, samplingrate = loadAudio(filename)\n",
    "lmfcc = mfcc(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['z', '4', '3']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recover the sequence of digits (word level transcription) in the file\n",
    "wordTrans = list(path2info(filename)[2])\n",
    "wordTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sil',\n",
       " 'z',\n",
       " 'iy',\n",
       " 'r',\n",
       " 'ow',\n",
       " 'sp',\n",
       " 'f',\n",
       " 'ao',\n",
       " 'r',\n",
       " 'sp',\n",
       " 'th',\n",
       " 'r',\n",
       " 'iy',\n",
       " 'sp',\n",
       " 'sil']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prondict import prondict\n",
    "phoneTrans = words2phones(wordTrans, prondict)\n",
    "phoneTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab2_proto import concatHMMs\n",
    "\n",
    "# Create a combined model for this specific utterance:\n",
    "utteranceHMM = concatHMMs(phoneHMMs, phoneTrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans for stateid in range(nstates[phone])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sil_0',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_2',\n",
       " 'z_0',\n",
       " 'z_0',\n",
       " 'z_0',\n",
       " 'z_0',\n",
       " 'z_1',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_1',\n",
       " 'iy_2',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_1',\n",
       " 'r_2',\n",
       " 'ow_0',\n",
       " 'ow_1',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'f_0',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_2',\n",
       " 'ao_0',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_1',\n",
       " 'r_2',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_1',\n",
       " 'th_1',\n",
       " 'th_1',\n",
       " 'th_2',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_1',\n",
       " 'r_2',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_1',\n",
       " 'iy_1',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lab2_tools import log_multivariate_normal_density_diag\n",
    "from lab2_proto import viterbi\n",
    "\n",
    "# NxM array of emission(observation) log likelihoods, N frames, M states\n",
    "obsloglik = log_multivariate_normal_density_diag(lmfcc, utteranceHMM['means'], utteranceHMM['covars']) \n",
    "log_startprob = np.log(utteranceHMM['startprob'][:-1])\n",
    "log_transmat = np.log(utteranceHMM['transmat'][:-1, :-1])\n",
    "vloglik, vpath = viterbi(obsloglik, log_startprob, log_transmat)\n",
    "\n",
    "stateList = [stateTrans[i] for i in vpath]\n",
    "stateList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lab1_proto import mspec\n",
    "\n",
    "# traindata = []\n",
    "# for root, dirs, files in os.walk('tidigits/disc_4.1.1/tidigits/train'):\n",
    "#     for file in files:\n",
    "#         if file.endswith('.wav'):\n",
    "#             filename = os.path.join(root, file)\n",
    "#             samples, samplingrate = loadAudio(filename)\n",
    "#             lmfcc = mfcc(samples)\n",
    "#             mspecs = mspec(samples)\n",
    "#             targets = forcedAlignment(lmfcc, phoneHMMs, phoneTrans)\n",
    "#             traindata.append({'filename': filename, 'lmfcc': lmfcc, 'mspec': mspecs, 'targets': targets})\n",
    "# np.savez('traindata.npz', traindata=traindata)\n",
    "\n",
    "# testdata = []\n",
    "# for root, dirs, files in os.walk('tidigits/disc_4.2.1/tidigits/test'):\n",
    "#     for file in files:\n",
    "#         if file.endswith('.wav'):\n",
    "#             filename = os.path.join(root, file)\n",
    "#             samples, samplingrate = loadAudio(filename)\n",
    "#             lmfcc = mfcc(samples)\n",
    "#             mspecs = mspec(samples)\n",
    "#             targets = forcedAlignment(lmfcc, phoneHMMs, phoneTrans)\n",
    "#             testdata.append({'filename': filename, 'lmfcc': lmfcc, 'mspec': mspecs, 'targets': targets})\n",
    "# np.savez('testdata.npz', testdata=testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('traindata.npz', allow_pickle=True)['traindata']\n",
    "random.seed(420)\n",
    "\n",
    "data_tuple_list = []\n",
    "for item in train_data:\n",
    "    filename = item['filename']\n",
    "    info_tuple = path2info(filename)\n",
    "    data_tuple_list.append(info_tuple)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "# Separate data by gender\n",
    "gender_data = defaultdict(list)\n",
    "for item in data_tuple_list:\n",
    "    gender_data[item[0]].append(item)\n",
    "\n",
    "train_set = []\n",
    "valid_set = []\n",
    "\n",
    "# Split data for each gender\n",
    "for gender, items in gender_data.items():\n",
    "    # Group by speaker\n",
    "    speaker_data = defaultdict(list)\n",
    "    for item in items:\n",
    "        speaker_data[item[1]].append(item)\n",
    "    \n",
    "    # Create lists of speakers and shuffle them to ensure randomness\n",
    "    speakers = list(speaker_data.keys())\n",
    "    random.shuffle(speakers)\n",
    "    \n",
    "    # Calculate the number of speakers to include in the training set\n",
    "    num_train_speakers = int(round(len(speakers) * 0.9))\n",
    "    \n",
    "    # Split the speakers into training and validation sets\n",
    "    train_speakers = speakers[:num_train_speakers]\n",
    "    valid_speakers = speakers[num_train_speakers:]\n",
    "    \n",
    "    # Aggregate the data entries corresponding to each set of speakers\n",
    "    for spkr in train_speakers:\n",
    "        train_set.extend(speaker_data[spkr])\n",
    "    for spkr in valid_speakers:\n",
    "        valid_set.extend(speaker_data[spkr])\n",
    "\n",
    "# Shuffle the training and validation sets to ensure random order\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('man', 'kd', '8', 'b'), ('man', 'gr', '5o25676', 'a'), ('man', 'bd', '6', 'b')]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
