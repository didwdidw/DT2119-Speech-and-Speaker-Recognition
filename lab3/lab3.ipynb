{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tim/Desktop/Speech/lab1')\n",
    "sys.path.append('/Users/tim/Desktop/Speech/lab2')\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from lab3_tools import *\n",
    "from lab3_proto import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sil_0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phoneHMMs is a dictionary with 21 keys, each corresponding to a phonetic model\n",
    "phoneHMMs = np.load('../lab2/lab2_models_all.npz', allow_pickle=True)['phoneHMMs'].item()\n",
    "phones = sorted(phoneHMMs.keys())\n",
    "nstates = {phone: phoneHMMs[phone]['means'].shape[0] for phone in phones}\n",
    "# A list of unique states for reference\n",
    "# Note that we model three segments for each phoneme\n",
    "stateList = [ph + '_' + str(id) for ph in phones for id in range(nstates[ph])]\n",
    "stateList[39]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forced Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the audio and compute liftered MFCC features\n",
    "from lab1_proto import mfcc\n",
    "\n",
    "filename = 'tidigits/disc_4.1.1/tidigits/train/man/nw/z43a.wav'\n",
    "samples, samplingrate = loadAudio(filename)\n",
    "lmfcc = mfcc(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['z', '4', '3']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recover the sequence of digits (word level transcription) in the file\n",
    "wordTrans = list(path2info(filename)[2])\n",
    "wordTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sil',\n",
       " 'z',\n",
       " 'iy',\n",
       " 'r',\n",
       " 'ow',\n",
       " 'sp',\n",
       " 'f',\n",
       " 'ao',\n",
       " 'r',\n",
       " 'sp',\n",
       " 'th',\n",
       " 'r',\n",
       " 'iy',\n",
       " 'sp',\n",
       " 'sil']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prondict import prondict\n",
    "phoneTrans = words2phones(wordTrans, prondict)\n",
    "phoneTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab2_proto import concatHMMs\n",
    "\n",
    "# Create a combined model for this specific utterance:\n",
    "utteranceHMM = concatHMMs(phoneHMMs, phoneTrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans for stateid in range(nstates[phone])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sil_0',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_1',\n",
       " 'sil_2',\n",
       " 'z_0',\n",
       " 'z_0',\n",
       " 'z_0',\n",
       " 'z_0',\n",
       " 'z_1',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'z_2',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_1',\n",
       " 'iy_2',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_1',\n",
       " 'r_2',\n",
       " 'ow_0',\n",
       " 'ow_1',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'ow_2',\n",
       " 'f_0',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_1',\n",
       " 'f_2',\n",
       " 'ao_0',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_1',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'ao_2',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_1',\n",
       " 'r_2',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_0',\n",
       " 'th_1',\n",
       " 'th_1',\n",
       " 'th_1',\n",
       " 'th_2',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_0',\n",
       " 'r_1',\n",
       " 'r_2',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_0',\n",
       " 'iy_1',\n",
       " 'iy_1',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'iy_2',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0',\n",
       " 'sil_0']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lab2_tools import log_multivariate_normal_density_diag\n",
    "from lab2_proto import viterbi\n",
    "\n",
    "# NxM array of emission(observation) log likelihoods, N frames, M states\n",
    "obsloglik = log_multivariate_normal_density_diag(lmfcc, utteranceHMM['means'], utteranceHMM['covars']) \n",
    "log_startprob = np.log(utteranceHMM['startprob'][:-1])\n",
    "log_transmat = np.log(utteranceHMM['transmat'][:-1, :-1])\n",
    "vloglik, vpath = viterbi(obsloglik, log_startprob, log_transmat)\n",
    "\n",
    "stateList = [stateTrans[i] for i in vpath]\n",
    "stateList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lab1_proto import mspec\n",
    "\n",
    "# traindata = []\n",
    "# for root, dirs, files in os.walk('tidigits/disc_4.1.1/tidigits/train'):\n",
    "#     for file in files:\n",
    "#         if file.endswith('.wav'):\n",
    "#             filename = os.path.join(root, file)\n",
    "#             samples, samplingrate = loadAudio(filename)\n",
    "#             lmfcc = mfcc(samples)\n",
    "#             mspecs = mspec(samples)\n",
    "#             targets = forcedAlignment(lmfcc, phoneHMMs, phoneTrans)\n",
    "#             traindata.append({'filename': filename, 'lmfcc': lmfcc, 'mspec': mspecs, 'targets': targets})\n",
    "# np.savez('traindata.npz', traindata=traindata)\n",
    "\n",
    "# testdata = []\n",
    "# for root, dirs, files in os.walk('tidigits/disc_4.2.1/tidigits/test'):\n",
    "#     for file in files:\n",
    "#         if file.endswith('.wav'):\n",
    "#             filename = os.path.join(root, file)\n",
    "#             samples, samplingrate = loadAudio(filename)\n",
    "#             lmfcc = mfcc(samples)\n",
    "#             mspecs = mspec(samples)\n",
    "#             targets = forcedAlignment(lmfcc, phoneHMMs, phoneTrans)\n",
    "#             testdata.append({'filename': filename, 'lmfcc': lmfcc, 'mspec': mspecs, 'targets': targets})\n",
    "# np.savez('testdata.npz', testdata=testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('traindata.npz', allow_pickle=True)['traindata']\n",
    "test_data = np.load('testdata.npz', allow_pickle=True)['testdata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(420)\n",
    "\n",
    "# Include original data with parsed information\n",
    "data_list = []\n",
    "for item in train_data:\n",
    "    filename = item['filename']\n",
    "    info_tuple = path2info(filename)\n",
    "    data_list.append((info_tuple, item))  # Store tuple of parsed info and the full data item\n",
    "\n",
    "from collections import defaultdict\n",
    "# Separate data by gender\n",
    "gender_data = defaultdict(list)\n",
    "for info, full_data in data_list:\n",
    "    gender_data[info[0]].append((info, full_data))\n",
    "\n",
    "\n",
    "train_set = []\n",
    "valid_set = []\n",
    "\n",
    "# Split data for each gender\n",
    "for gender, items in gender_data.items():\n",
    "    # Group by speaker\n",
    "    speaker_data = defaultdict(list)\n",
    "    for info, full_data in items:\n",
    "        speaker_data[info[1]].append(full_data)\n",
    "    \n",
    "    # Create lists of speakers and shuffle them\n",
    "    speakers = list(speaker_data.keys())\n",
    "    random.shuffle(speakers)\n",
    "    \n",
    "    # Calculate number of speakers for training\n",
    "    num_train_speakers = int(round(len(speakers) * 0.9))\n",
    "    \n",
    "    # Split speakers into training and validation\n",
    "    train_speakers = speakers[:num_train_speakers]\n",
    "    valid_speakers = speakers[num_train_speakers:]\n",
    "    \n",
    "    # Aggregate the data entries for each set of speakers\n",
    "    for spkr in train_speakers:\n",
    "        train_set.extend(speaker_data[spkr])\n",
    "    for spkr in valid_speakers:\n",
    "        valid_set.extend(speaker_data[spkr])\n",
    "\n",
    "# Shuffle the sets to ensure random order\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acoustic Context (Dynamic Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlmfcc_train_x = []\n",
    "dmspec_train_x = []\n",
    "for i in range(len(train_set)):\n",
    "    current_utterance = train_set[i]\n",
    "    num_timesteps = current_utterance['lmfcc'].shape[0]\n",
    "\n",
    "    for t in range(num_timesteps):\n",
    "        temp_mfcc_stack = []\n",
    "        temp_mspec_features = []\n",
    "        for j in range(t - 3, t + 3 + 1):\n",
    "            if j < 0:\n",
    "                # Mirror at the beginning\n",
    "                temp_mfcc_stack.extend(current_utterance['lmfcc'][abs(j)])\n",
    "                temp_mspec_features.extend(current_utterance['mspec'][abs(j)])\n",
    "            elif j >= num_timesteps:\n",
    "                # Mirror at the end\n",
    "                temp_mfcc_stack.extend(current_utterance['lmfcc'][2 * num_timesteps - j - 1])\n",
    "                temp_mspec_features.extend(current_utterance['mspec'][2 * num_timesteps - j - 1])\n",
    "            else:\n",
    "                # Normal case\n",
    "                temp_mfcc_stack.extend(current_utterance['lmfcc'][j])\n",
    "                temp_mspec_features.extend(current_utterance['mspec'][j])\n",
    "        \n",
    "        # Flatten the data structures as specified at 4.6\n",
    "        dlmfcc_train_x.append(temp_mfcc_stack)\n",
    "        dmspec_train_x.append(temp_mspec_features)\n",
    "\n",
    "dlmfcc_train_x = np.array(dlmfcc_train_x)\n",
    "dmspec_train_x = np.array(dmspec_train_x) \n",
    "\n",
    "#---------------------------------------------------------#\n",
    "dlmfcc_val_x = []\n",
    "dmspec_val_x = []\n",
    "for i in range(len(valid_set)):\n",
    "    current_utterance = valid_set[i]\n",
    "    num_timesteps = current_utterance['lmfcc'].shape[0]\n",
    "\n",
    "    for t in range(num_timesteps):\n",
    "        temp_mfcc_stack = []\n",
    "        temp_mspec_features = []\n",
    "        for j in range(t - 3, t + 3 + 1):\n",
    "            if j < 0:\n",
    "                # Mirror at the beginning\n",
    "                temp_mfcc_stack.extend(current_utterance['lmfcc'][abs(j)])\n",
    "                temp_mspec_features.extend(current_utterance['mspec'][abs(j)])\n",
    "            elif j >= num_timesteps:\n",
    "                # Mirror at the end\n",
    "                temp_mfcc_stack.extend(current_utterance['lmfcc'][2 * num_timesteps - j - 1])\n",
    "                temp_mspec_features.extend(current_utterance['mspec'][2 * num_timesteps - j - 1])\n",
    "            else:\n",
    "                # Normal case\n",
    "                temp_mfcc_stack.extend(current_utterance['lmfcc'][j])\n",
    "                temp_mspec_features.extend(current_utterance['mspec'][j])\n",
    "        \n",
    "        # Flatten the data structures as specified at 4.6\n",
    "        dlmfcc_val_x.append(temp_mfcc_stack)\n",
    "        dmspec_val_x.append(temp_mspec_features)\n",
    "\n",
    "dlmfcc_val_x = np.array(dlmfcc_val_x)\n",
    "dmspec_val_x = np.array(dmspec_val_x) \n",
    "    \n",
    "#---------------------------------------------------------#\n",
    "dlmfcc_test_x = []\n",
    "dmspec_test_x = []\n",
    "for i in range(len(train_set)):\n",
    "    current_utterance = train_set[i]\n",
    "    num_timesteps = current_utterance['lmfcc'].shape[0]\n",
    "\n",
    "    for t in range(num_timesteps):\n",
    "        temp_mfcc_stack = []\n",
    "        temp_mspec_features = []\n",
    "        for j in range(t - 3, t + 3 + 1):\n",
    "            if j < 0:\n",
    "                # Mirror at the beginning\n",
    "                temp_mfcc_stack.extend(current_utterance['lmfcc'][abs(j)])\n",
    "                temp_mspec_features.extend(current_utterance['mspec'][abs(j)])\n",
    "            elif j >= num_timesteps:\n",
    "                # Mirror at the end\n",
    "                temp_mfcc_stack.extend(current_utterance['lmfcc'][2 * num_timesteps - j - 1])\n",
    "                temp_mspec_features.extend(current_utterance['mspec'][2 * num_timesteps - j - 1])\n",
    "            else:\n",
    "                # Normal case\n",
    "                temp_mfcc_stack.extend(current_utterance['lmfcc'][j])\n",
    "                temp_mspec_features.extend(current_utterance['mspec'][j])\n",
    "        \n",
    "        # Flatten the data structures as specified at 4.6\n",
    "        dlmfcc_test_x.append(temp_mfcc_stack)\n",
    "        dmspec_test_x.append(temp_mspec_features)\n",
    "\n",
    "dlmfcc_test_x = np.array(dlmfcc_test_x)\n",
    "dmspec_test_x = np.array(dmspec_test_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What will happen with the very short utterances in the files containing isolated digits when normalizing each utterance individually?\n",
    "\n",
    "Short utterances will have less reliable estimates of mean and variance, leading to potentially unstable feature scaling.\n",
    "\n",
    "Extreme normalization values might occur if a particular feature deviates slightly in a short utterance, as there's less data to average out noise and variability. This can exaggerate the importance of minor variations in short utterances, potentially skewing the model training or performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmfcc_train_x = []\n",
    "mspec_train_x = []\n",
    "train_y = []\n",
    "for i in range(len(train_set)):\n",
    "    lmfcc_train_x.append(train_set[i]['lmfcc'])\n",
    "    mspec_train_x.append(train_set[i]['mspec'])\n",
    "    train_y.append(train_set[i]['targets'])\n",
    "lmfcc_train_x = np.concatenate(lmfcc_train_x, axis=0)\n",
    "mspec_train_x = np.concatenate(mspec_train_x, axis=0)\n",
    "train_y = np.concatenate(train_y, axis=0)\n",
    "#---------------------------------------------------------#\n",
    "lmfcc_val_x = []\n",
    "mspec_val_x = []\n",
    "val_y = []\n",
    "for i in range (len(valid_set)):\n",
    "    lmfcc_val_x.append(valid_set[i]['lmfcc'])\n",
    "    mspec_val_x.append(valid_set[i]['mspec'])\n",
    "    val_y.append(valid_set[i]['targets'])\n",
    "lmfcc_val_x = np.concatenate(lmfcc_val_x, axis=0)\n",
    "mspec_val_x = np.concatenate(mspec_val_x, axis=0)\n",
    "val_y = np.concatenate(val_y, axis=0)\n",
    "#---------------------------------------------------------#\n",
    "lmfcc_test_x = []\n",
    "mspec_test_x = []\n",
    "test_y = []\n",
    "for i in range (len(test_data)):\n",
    "    lmfcc_test_x.append(test_data[i]['lmfcc'])\n",
    "    mspec_test_x.append(test_data[i]['mspec'])\n",
    "    test_y.append(test_data[i]['targets'])\n",
    "lmfcc_test_x = np.concatenate(lmfcc_test_x, axis=0)\n",
    "mspec_test_x = np.concatenate(mspec_test_x, axis=0)\n",
    "test_y = np.concatenate(test_y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1363305, 13)\n",
      "(144087, 13)\n",
      "(1527014, 13)\n",
      "\n",
      "(1363305, 40)\n",
      "(144087, 40)\n",
      "(1527014, 40)\n",
      "\n",
      "(1363305, 91)\n",
      "(144087, 91)\n",
      "(1363305, 91)\n",
      "\n",
      "(1363305, 280)\n",
      "(144087, 280)\n",
      "(1363305, 280)\n"
     ]
    }
   ],
   "source": [
    "print(lmfcc_train_x.shape)\n",
    "print(lmfcc_val_x.shape)\n",
    "print(lmfcc_test_x.shape)\n",
    "print()\n",
    "print(mspec_train_x.shape)\n",
    "print(mspec_val_x.shape)\n",
    "print(mspec_test_x.shape)\n",
    "print()\n",
    "print(dlmfcc_train_x.shape)\n",
    "print(dlmfcc_val_x.shape)\n",
    "print(dlmfcc_test_x.shape)\n",
    "print()\n",
    "print(dmspec_train_x.shape)\n",
    "print(dmspec_val_x.shape)\n",
    "print(dmspec_test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lmfcc_scaler = StandardScaler()\n",
    "dlmfcc_scaler = StandardScaler()\n",
    "mspec_scaler = StandardScaler()\n",
    "dmspec_scaler = StandardScaler()\n",
    "\n",
    "lmfcc_train_x = lmfcc_scaler.fit_transform(lmfcc_train_x)\n",
    "dlmfcc_train_x = dlmfcc_scaler.fit_transform(dlmfcc_train_x)\n",
    "mspec_train_x = mspec_scaler.fit_transform(mspec_train_x)\n",
    "dmspec_train_x = dmspec_scaler.fit_transform(dmspec_train_x)\n",
    "lmfcc_train_x = lmfcc_train_x.astype('float32')\n",
    "dlmfcc_train_x = dlmfcc_train_x.astype('float32')\n",
    "mspec_train_x = mspec_train_x.astype('float32')\n",
    "dmspec_train_x = dmspec_train_x.astype('float32')\n",
    "\n",
    "# Transform validation and test data using the normalization coefficients from training data\n",
    "lmfcc_val_x = lmfcc_scaler.transform(lmfcc_val_x)\n",
    "dlmfcc_val_x = dlmfcc_scaler.transform(dlmfcc_val_x)\n",
    "mspec_val_x = mspec_scaler.transform(mspec_val_x)\n",
    "dmspec_val_x = dmspec_scaler.transform(dmspec_val_x)\n",
    "lmfcc_val_x = lmfcc_val_x.astype('float32')\n",
    "dlmfcc_val_x = dlmfcc_val_x.astype('float32')\n",
    "mspec_val_x = mspec_val_x.astype('float32')\n",
    "dmspec_val_x = dmspec_val_x.astype('float32')\n",
    "\n",
    "lmfcc_test_x = lmfcc_scaler.transform(lmfcc_test_x)\n",
    "dlmfcc_test_x = dlmfcc_scaler.transform(dlmfcc_test_x)\n",
    "mspec_test_x = mspec_scaler.transform(mspec_test_x)\n",
    "dmspec_test_x = dmspec_scaler.transform(dmspec_test_x)\n",
    "lmfcc_test_x = lmfcc_test_x.astype('float32')\n",
    "dlmfcc_test_x = dlmfcc_test_x.astype('float32')\n",
    "mspec_test_x = mspec_test_x.astype('float32')\n",
    "dmspec_test_x = dmspec_test_x.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = len(stateList)\n",
    "train_y = F.one_hot(torch.tensor(train_y), num_classes=output_dim)\n",
    "val_y = F.one_hot(torch.tensor(val_y), num_classes=output_dim)\n",
    "test_y = F.one_hot(torch.tensor(test_y), num_classes=output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoneme Recognition with Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DNN model class\n",
    "class PhonemeDNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_layers=1, num_units=256):\n",
    "        super(PhonemeDNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, num_units))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_units, num_units))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(num_units, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "input_size = 13  # Assuming MFCC features of dimension 13\n",
    "num_classes = 10  # Total number of phoneme classes\n",
    "num_layers = 2    # Number of hidden layers\n",
    "\n",
    "# Load your data into DataLoader\n",
    "# train_loader and val_loader should be defined with your actual data\n",
    "# model = PhonemeDNN(input_size, num_classes, num_layers)\n",
    "# train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Implement similar setup for 1 to 4 layers, and for different features (MFCC, filterbank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the influence of feature kind and size of input context window?\n",
    "The model performance. \n",
    "\n",
    "The context window size determines how much of the temporal information surrounding each frame is considered by the network. A larger window provides more contextual information, potentially improving recognition accuracy but also increasing the complexity of the model. Dynamic features, which include several frames of context, typically lead to better performance compared to static features.\n",
    "\n",
    "### What is the purpose of normalising (standardising) the input feature vectors depending on the activation functions in the network?\n",
    "Normalising the feature vectors ensures that each feature contributes equally to the learning process, preventing features with larger numerical ranges from dominating the training dynamics. \n",
    "\n",
    "This is crucial when using activation functions like sigmoid or tanh in the network, as these functions are sensitive to input scale and can suffer from saturation issues: inputs with large absolute values cause the function outputs to be near the function's asymptotes, resulting in gradients close to zero.\n",
    "\n",
    "### What is the influence of the number of units per layer and the number of layers?\n",
    "Increasing the number of units per layer typically allows the network to learn more complex patterns and representations, improving its capability to differentiate between more subtle features of the input. \n",
    "\n",
    "Similarly, more layers can enable deeper (hence, potentially more abstract) feature learning. \n",
    "\n",
    "However, larger and deeper networks are more prone to overfitting and require more data and computational resources to train effectively.\n",
    "\n",
    "### What is the influence of the activation function (when you try other activation functions than ReLU, you do not need to reach convergence in case you do not have enough time)\n",
    "The choice of activation function affects the training dynamics and the performance of the network. \n",
    "\n",
    "ReLU (Rectified Linear Unit) is commonly used because it helps in alleviating the vanishing gradient problem encountered with sigmoid or tanh functions. \n",
    "\n",
    "Trying other activation functions like Leaky ReLU, ELU, or sigmoid might affect convergence speed and the final model performance, especially if the network architecture or data characteristics make it susceptible to issues like dying ReLU problem or gradient vanishing/explosion.\n",
    "\n",
    "### What is the influence of the learning rate/learning rate strategy?\n",
    "Too high a rate can cause the training to diverge, while too low a rate might result in a painfully slow convergence or getting stuck in local minima. \n",
    "\n",
    "Adaptive learning rate strategies like Adam adjust the learning rate during training, which can lead to faster convergence and can alleviate some of the tuning requirements.\n",
    "\n",
    "### How stable are the posteriograms from the network in time?\n",
    "Posteriors produced by the network should ideally be stable for consistent phonetic units across similar contexts. \n",
    "\n",
    "However, in practice, stability can vary depending on factors like network architecture, training sufficiency, and the inherent variability of speech. Observing fluctuations in posteriograms can indicate issues with model generalization or insufficient training.\n",
    "\n",
    "### How do the errors distribute depending on phonetic class?\n",
    "Errors in phoneme recognition often vary by phonetic class. \n",
    "\n",
    "Some phonemes might be consistently harder to recognize due to their acoustic similarity to other phonemes, less distinctiveness in their spectral features, or their shorter duration. \n",
    "\n",
    "Confusion matrices by phonetic class can help identify which phonemes are most frequently misclassified and which phonemes they are confused with, guiding further model adjustments or targeted data augmentation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
